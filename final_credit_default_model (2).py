# -*- coding: utf-8 -*-
"""FINAL_CREDIT DEFAULT MODEL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_GAUAmSIC36SEoxsCn-Ju1Nvhgs6VuOs
"""

!pip uninstall -y imbalanced-learn && pip install -U imbalanced-learn

# =========================
# CREDIT DEFAULT PREDICTION
# =========================

# Step 1: Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE

# Step 2: Load Datafrom google.colab import files
from google.colab import files
uploaded = files.upload()  # Upload your Excel file

data = pd.read_excel('Train_Dataset__.xlsx')

# Step 3: Initial EDA
print("Initial Data Info:")
print(data.info())
print("\nFirst 5 rows:")
print(data.head())
print("\nMissing values per column:")
print(data.isnull().sum())

# Plot histograms for all numeric variables
import math
numeric_cols = data.select_dtypes(include=np.number).columns.tolist()
cols = 4
rows = math.ceil(len(numeric_cols)/cols)
plt.figure(figsize=(5*cols, 4*rows))
for i, col in enumerate(numeric_cols):
    plt.subplot(rows, cols, i+1)
    sns.histplot(data[col], bins=30, color='skyblue', kde=True)
    plt.title(col)
plt.tight_layout()
plt.show()

# Check skewness
print("\nSkewness before transformation:")
print(data[numeric_cols].skew())

# --- Step 4: Data Preprocessing ---
data.drop(columns=['ID', 'ID_Days', 'Own_House_Age'], inplace=True, errors='ignore')

cols_to_convert = ['Client_Income', 'Credit_Amount', 'Loan_Annuity', 'Age_Days',
                   'Employed_Days', 'Registration_Days', 'Application_Process_Day',
                   'Application_Process_Hour', 'Phone_Change', 'Score_Source_3']
for col in cols_to_convert:
    if col in data.columns:
        data[col] = pd.to_numeric(data[col], errors='coerce')

numeric_cols = data.select_dtypes(include=np.number).columns.tolist()
categorical_cols = data.select_dtypes(include='object').columns.tolist()

# Impute missing values
for col in numeric_cols:
    if data[col].isnull().sum() > 0:
        data[col] = data[col].fillna(data[col].median()) # Corrected line
for col in categorical_cols:
    if data[col].isnull().sum() > 0:
        data[col] = data[col].fillna(data[col].mode()[0]) # Corrected line

# Step 5- Feature Engineering
# -------------------------
# Debt-to-Income Ratio
data['Debt_to_Income_Ratio'] = data['Loan_Annuity'] / data['Client_Income']

# Convert days to years
data['Employed_Years'] = np.abs(data['Employed_Days']) / 365
data['Age_Years'] = data['Age_Days'] / 365

# -------------------------
# Log-Transform Highly Skewed Variables
# -------------------------
# Check skewness again for numeric variables
skewed_features = data[numeric_cols].skew().sort_values(ascending=False)
print("\nSkewness sorted before transformation:")
print(skewed_features)

# Transform only highly skewed positive variables
if 'Client_Income' in skewed_features.index and skewed_features['Client_Income'] > 1:
    data['Client_Income_Log'] = np.log1p(data['Client_Income'])  # log1p handles 0s safely

# -------------------------
# Steep 6 - EDA After Transformation
# -------------------------
import matplotlib.pyplot as plt
import seaborn as sns

# Variables from hypothesis
hypothesis_vars = ['Client_Income_Log', 'Employed_Days', 'Score_Source_2',
                   'Debt_to_Income_Ratio', 'Employed_Years', 'Age_Years']

# Plot histograms for hypothesis variables
plt.figure(figsize=(15,10))
for i, col in enumerate(hypothesis_vars):
    plt.subplot(2, 3, i+1)
    sns.histplot(data[col], bins=30, color='green', kde=True)
    plt.title(col)
plt.tight_layout()
plt.show()

# Check skewness after transformation
print("\nSkewness after transformation:")
print(data[hypothesis_vars].skew())

# Correlation heatmap for hypothesis variables
plt.figure(figsize=(8,6))
sns.heatmap(data[hypothesis_vars].corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Heatmap (Hypothesis Variables)')
plt.show()

# Import Libraries and Load Data ---
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,average_precision_score

# Define features and target
features = ['Client_Income_Log', 'Score_Source_2',
            'Debt_to_Income_Ratio', 'Employed_Years', 'Age_Years', 'Credit_Amount']
X = data[features]
y = data['Default']

# --- Step 3: Implement Cross-Validation with SMOTE in a Pipeline for all Models ---

# Define the models to evaluate
models = {
    'Logistic Regression': LogisticRegression(solver='liblinear', random_state=123),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=123, n_jobs=-1)
}

# Create Stratified K-Fold cross-validator
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)

# Dictionary to store results for each model
results = {name: {'auc': [], 'precision': [], 'recall': [], 'f1': [], 'pr_auc': [],'probas': [], 'y_tests': [], } for name in models.keys()}

# Loop through each model
for model_name, model in models.items():
    print(f"--- Running Cross-Validation for {model_name} ---")

    # Create a pipeline that combines scaling, SMOTE, and the current model
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('smote', SMOTE(random_state=123)),
        ('classifier', model)
    ])

    # Loop through each fold
    for fold, (train_index, test_index) in enumerate(skf.split(X, y)):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # Fit the pipeline on the training data for this fold
        pipeline.fit(X_train, y_train)

        # Get predictions on the test data for this fold
        y_pred = pipeline.predict(X_test)
        y_prob = pipeline.predict_proba(X_test)[:, 1]

         # Store data for plotting ROC curve
        results[model_name]['probas'].extend(y_prob)
        results[model_name]['y_tests'].extend(y_test)

        # Calculate and store metrics for this fold
        results[model_name]['auc'].append(roc_auc_score(y_test, y_prob))
        results[model_name]['precision'].append(precision_score(y_test, y_pred))
        results[model_name]['recall'].append(recall_score(y_test, y_pred))
        results[model_name]['f1'].append(f1_score(y_test, y_pred))
        results[model_name]['pr_auc'].append(average_precision_score(y_test, y_prob))

# --- Step 4: Display the Final Results ---
print("\n" + "="*50)
print("             Final Cross-Validation Results")
print("="*50)

for model_name, metrics in results.items():
    print(f"\n--- {model_name} ---")
    print(f"  Average AUC: {np.mean(metrics['auc']):.4f} (Std: {np.std(metrics['auc']):.4f})")
    print(f"  Average Precision: {np.mean(metrics['precision']):.4f} (Std: {np.std(metrics['precision']):.4f})")
    print(f"  Average Recall: {np.mean(metrics['recall']):.4f} (Std: {np.std(metrics['recall']):.4f})")
    print(f"  Average F1-Score: {np.mean(metrics['f1']):.4f} (Std: {np.std(metrics['f1']):.4f})")
    print(f"  Average PR-AUC: {np.mean(metrics['pr_auc']):.4f} (Std: {np.std(metrics['pr_auc']):.4f})")

from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, average_precision_score, roc_curve, auc

# --- Step 5: Plot ROC Curve for all Models ---
plt.figure(figsize=(10, 8))
for model_name, res in results.items():
    fpr, tpr, _ = roc_curve(res['y_tests'], res['probas'])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.4f})')

plt.plot([0, 1], [0, 1], 'k--', label='Baseline')
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve for All Models', fontsize=16)
plt.legend(loc='lower right')
plt.grid(True)
plt.tight_layout()
plt.savefig('all_models_roc_curve.png')
print("\nROC curve plot has been generated and saved.")

# --- Step 6: Feature Importance for Random Forest ---
# Retrain the pipeline on the full dataset for a more stable feature importance
rf_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=123)),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=123, n_jobs=-1))
])
rf_pipeline.fit(X, y)
rf_classifier = rf_pipeline.named_steps['classifier']
feature_importances = rf_classifier.feature_importances_
feature_names = X.columns
importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})
importance_df = importance_df.sort_values('importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='importance', y='feature', data=importance_df, palette='viridis')
plt.title('Random Forest Feature Importance', fontsize=16)
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.tight_layout()
plt.savefig('rf_feature_importance.png')
print("Random Forest feature importance plot has been generated and saved.")

#LOGISTIC REGRESSION ANALYSIS

from imblearn.pipeline import Pipeline   # <-- use this one!
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import SMOTE

# Build pipeline only for Logistic Regression
logit_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=123)),
    ('classifier', LogisticRegression(solver='liblinear', random_state=123))
])

# Fit
logit_pipeline.fit(X, y)

import pandas as pd
import numpy as np

coefs = logit_pipeline.named_steps['classifier'].coef_[0]
feature_names = features

summary = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient (log-odds)': coefs,
    'Odds Ratio': np.exp(coefs)
}).sort_values(by='Odds Ratio', ascending=False)

print(summary)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# --- Step 1: Put your results into a DataFrame ---
data = {
    "Feature": [
        "Debt_to_Income_Ratio",
        "Client_Income_Log",
        "Age_Years",
        "Credit_Amount",
        "Employed_Years",
        "Score_Source_2"
    ],
    "Coefficient (log-odds)": [
        0.172678,
        0.109652,
        -0.059909,
        -0.173628,
        -0.264206,
        -1.935026
    ]
}

df = pd.DataFrame(data)

# --- Step 2: Calculate Odds Ratios ---
df["Odds Ratio"] = np.exp(df["Coefficient (log-odds)"])

# --- Step 3: Plot Odds Ratios ---
plt.figure(figsize=(8, 5))
bars = plt.barh(df["Feature"], df["Odds Ratio"], color=["Darkorange" if x>1 else "yellow" for x in df["Odds Ratio"]])
plt.axvline(x=1, color="black", linestyle="--")  # Reference line at OR=1

# Annotate values
for bar in bars:
    width = bar.get_width()
    plt.text(width, bar.get_y() + bar.get_height()/2,
             f"{width:.2f}", va='center', ha="left", fontsize=10)

plt.title("Odds Ratios of Features", fontsize=14)
plt.xlabel("Odds Ratio")
plt.ylabel("Features")
plt.tight_layout()
plt.show()

# --- Step 4: Show final DataFrame ---
print(df)